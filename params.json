{
  "name": "untitled",
  "tagline": "advanced twitter research",
  "body": "# untitled -- advanced twitter research \r\n\r\n### what is it \r\n\r\nuntitled is an open-source solution for advanced analysis of high volume keywords in Twitter. The basic premise is that you choose 1 or more keywords and get rich and very high quality signal taxonomy with over 50 signals in clean csv format. I've left the analysis for you. \r\n\r\nTwitter data can be analysed from three different \"point of views\". \r\n\r\n1) network activity\r\n\r\n2) user \r\n\r\n3) content\r\n\r\nAt the moment these scripts are focused on network activity and user. There is one output format where a row represents an aggregate of the network in a given time interval, and other where a row represents stats for a given user. \r\n\r\n(content summary tables will be added later)\r\n\r\n\r\n### getting started \r\n\r\n1. Configuration\r\n  \r\n  1.1 setting up keywords\r\n\r\n  1.2 setting up and/or configuring api access keys \r\n  \r\n  1.3 setting up upstart or similar (or crontab)\r\n  \r\n  1.4 configure network analysis complexity [OPTIONAL]\r\n    \r\n2. Process cycle\r\n\r\n  2.1 start collecting tweets (start the upstart service) \r\n  \r\n  2.2 start the main process (execute run.sh)\r\n  \r\n  2.3 access outputs in .csv files named according to the keywords \r\n  \r\n\r\nNOTE: Step 2.2 and 2.3 might take some time depending on the size of your dataset. There will be two separate .csv files for each keyword\r\n\r\n- user level data  (for all users tweeting the topic) \r\n- network level data (aggregate for the keyword in 10 minute intervals)\r\n\r\n\r\n### what can be configured? \r\n\r\n- keywords\r\n- network statistics time interval (default is 10 minutes) \r\n- network graph analysis complexity\r\n- api keys\r\n- upstart or similar / crontab \r\n\r\n\r\n### server configuration\r\n\r\nOther than setting up upstart on the server that takes care of the TwitterApi streaming API connection for collecting tweets\r\n\r\n\r\n### dependencies \r\n\r\n    ruby gem install t\r\n    pip install twitter\r\n    pip install TwitterAPI \r\n\r\ntwitter will be used for user related queries. TwitterAPI for streaming (fetching tweets in an on-going manner from the Twitter Streaming API), and t for network graph related searches. \r\n\r\n\r\n##### IMPORTANT: For the current configuration, you'll need 15 Twitter API accounts. \r\n\r\nThis has nothing to do with the fact that we're using more than one API wrapper to access Twitter data. \r\n\r\n4 -> twitter for user searches\r\n\r\n1 -> TwitterApi for streaming\r\n\r\n10 -> t for network graph analysis \r\n\r\n Further note on API use...industrial social media listening platforms may use hundreds (or more) Twitter API keys at any given point in time. I question the merits of such an approach for commercial purpose, but we are solely interested in research here. \r\n\r\n#### the scripts \r\n\r\n##### run.sh \r\n\r\nThe main controller script that runs the program. \r\n\r\n#####untitled-stream.py\r\n\r\nHandles tweet collection. This process should be added ideally to upstart or similar. Crontab may do as well, but upstart or similar is much better. \r\n\r\n##### untitled-graph.sh\r\n\r\nHandles network graph analysis. Note that this is not scalable at all in the current state, but gives an indication of the 3rd tier graph quality. \r\n\r\n##### untitled-network.sh\r\n\r\nThis is where most of the functions reside. All of the functions related to \"network\" aspect can be found here. \r\n\r\n##### untitled-users.sh\r\n\r\nThe main script handling users API calls to Twitter and the processing of the data that comes in return.  \r\n\r\n##### untitled-users.py\r\n\r\nThe script that handles that actual Twitter API connection. \r\n\r\n##### untitled-entropy.r \r\n\r\nVery simple R script that estimates Shannon entropy of a string\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}